{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":269359,"sourceType":"datasetVersion","datasetId":111880}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model compression\n\nDeep neural networks are the state-of-the-art models for many tasks, especially with unstructured data like images, text etc. One issue with deep learning models is that they are highly overparamterized, making it difficult to deploy/use it in devices with low memory requirements. Compress the neural networks model without sacrifying the model performance is an active reasearch topic in machine learning. Parameter pruning,quantization,knowledge distillation etc  are the widely used approaches nowadays to make the neural networks compact. In this notebook, we will demonstrate a parameter pruning technique on a fine-tuned Resnet50 model. \n\n## Model Pruning\n\nPruning is a technique used for reducing redundant parameters which are not sensitive to the performance. One way to reduce parameters while training is by using L-1 regularisation. But when using L1 regularisation, there is not much control over the number of parameters that needs to be pruned.  There were earlier works in deep learning that tried out various pruning techniques, for example by Yann Le Cun et. al, in the paper [**Optimal Brain Damage**](https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html). The work tried to reduce the number of connections based on the Hessian of the loss function. Nowadays there exist different methods for pruning a neural networks. There are approaches that take care of sparsity while training the model like [**winning lottery ticket hypothesis**](https://arxiv.org/abs/1803.03635) and more hardware-efficient pruning methods like [**N:M Fine-grained Structured Sparse Neural Networks**](https://arxiv.org/abs/2102.04010). In his notebook, we will demonstrate a much simpler pruning method called **magnitude weight pruning.**\n\n### Magnitude Weight pruning\n\nRef : https://arxiv.org/pdf/1506.02626.pdf\n\nIn Magnitude weight pruning technique, weights based on desired sparsity level is pruned from a neural network after training. The assumption is that weights with larger absolute values are important. In magnitude pruning, if we want 'X%' sparsity, we prune the smaller weights based on the absolute value to attain the desired sparsity. This can be done layer-wise (local), or on entire-model (global). While doing global pruning, one must ensure that not all the weights in a single layer is pruned, which affects the layerwise communication in neural network. To improve the model performance, the model is often fine-tuned with existing weights. If this process is carried iteratively, it is called \"**iterative magnitude pruning**\". There are many different ways to prune the weights like pattern-based pruning, vector-based pruning, vector-level pruning, kernel-level pruning, channel-level pruning etc but they are beyond the scope of this notebook. To make the pruning effective and hardware efficient, one may have to use some of these pruning techniques than the simple magnitude-based pruning.\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Problem statement\n\nThis is a multiclass image classification problem. There data contains images from 6 categories 'buildings','forest','glacier','mountain','sea','street'. The aim is to develop a machine learning model that correctly classifies an input image into one of the categories.\n\nIn this notebook, we try to find a model with a fixed % of sparsity, without lossing much expressivity of the original dense model.","metadata":{}},{"cell_type":"markdown","source":"## Modeling Approach : Transfer learning\n\nRef : https://www.kaggle.com/code/darraghcaffrey/transfer-learning-with-resnet50-91-5-test-acc <br>\n      https://arxiv.org/abs/1512.03385\n\nWe use pretrained Resnet-50 model and finetune on the trainset. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import OrderedDict\nimport cv2\nimport os\nfrom PIL import Image\n\n\nimport torch\nfrom torch import optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import random_split, DataLoader\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision.utils import make_grid\nfrom torchvision import transforms, models, datasets\nimport torch.nn.utils.prune as prune","metadata":{"execution":{"iopub.status.busy":"2024-06-30T08:57:28.510275Z","iopub.execute_input":"2024-06-30T08:57:28.510674Z","iopub.status.idle":"2024-06-30T08:57:31.054265Z","shell.execute_reply.started":"2024-06-30T08:57:28.510627Z","shell.execute_reply":"2024-06-30T08:57:31.053214Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dir = \"../input/intel-image-classification/seg_train/seg_train/\"\ntest_dir = \"../input/intel-image-classification/seg_test/seg_test/\"\npred_dir =\"../input/intel-image-classification/seg_pred/seg_pred/\"\n\n\npred_files = [os.path.join(pred_dir, f) for f in os.listdir(pred_dir)]","metadata":{"execution":{"iopub.status.busy":"2024-06-30T08:57:31.056128Z","iopub.execute_input":"2024-06-30T08:57:31.056577Z","iopub.status.idle":"2024-06-30T08:57:31.081569Z","shell.execute_reply.started":"2024-06-30T08:57:31.056549Z","shell.execute_reply":"2024-06-30T08:57:31.080683Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cat_counts = {}\nfor cat in os.listdir(train_dir):\n    counts = len(os.listdir(os.path.join(train_dir, cat)))\n    cat_counts[cat] =counts\nprint(cat_counts)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T08:57:31.082778Z","iopub.execute_input":"2024-06-30T08:57:31.083127Z","iopub.status.idle":"2024-06-30T08:57:31.097628Z","shell.execute_reply.started":"2024-06-30T08:57:31.083092Z","shell.execute_reply":"2024-06-30T08:57:31.096787Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# using mean and std for which Resnet was trained on\nmean = [0.485, 0.456, 0.406] \nstd = [0.229, 0.224, 0.225]","metadata":{"execution":{"iopub.status.busy":"2024-06-30T08:57:31.099511Z","iopub.execute_input":"2024-06-30T08:57:31.10026Z","iopub.status.idle":"2024-06-30T08:57:31.104628Z","shell.execute_reply.started":"2024-06-30T08:57:31.100229Z","shell.execute_reply":"2024-06-30T08:57:31.103896Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Torchvision train and test transforms \ntrain_transforms = transforms.Compose([transforms.Resize((150, 150)), # Resize all images \n                                       transforms.RandomResizedCrop(150),# Crop\n                                       transforms.RandomRotation(30), # Rotate \n                                       transforms.RandomHorizontalFlip(), # Flip\n                                       transforms.ToTensor(), # Convert\n                                       transforms.Normalize(torch.Tensor(mean), torch.Tensor(std)) # Normalize\n                                       ])\n\n\n\ntest_transforms = transforms.Compose([transforms.Resize((150, 150)),\n                                     transforms.CenterCrop(150),\n                                     transforms.ToTensor(),\n                                     transforms.Normalize(torch.Tensor(mean),torch.Tensor(std))\n                                     ])\n\n# Tmp torchvision datasets.Image folder to split into train and validation sets\ntmp_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n# len(tmp_data): 14034\n\n# Randomsplit tmp data based on length of dataset and set seed for reproducable split\ntrain_data, val_data = random_split(tmp_data, [10000, 4034], generator=torch.Generator().manual_seed(42))\n# Test set with with test transforms \ntest_data = datasets.ImageFolder(test_dir, transform=test_transforms)\n\n\n# Set Pytorch dataloaders, batch_size, training set shuffle\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\nval_loader = torch.utils.data.DataLoader(val_data, batch_size=64)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T08:57:31.106124Z","iopub.execute_input":"2024-06-30T08:57:31.107015Z","iopub.status.idle":"2024-06-30T08:57:39.495931Z","shell.execute_reply.started":"2024-06-30T08:57:31.106983Z","shell.execute_reply":"2024-06-30T08:57:39.494915Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T08:57:39.49724Z","iopub.execute_input":"2024-06-30T08:57:39.497541Z","iopub.status.idle":"2024-06-30T08:57:39.523754Z","shell.execute_reply.started":"2024-06-30T08:57:39.497515Z","shell.execute_reply":"2024-06-30T08:57:39.522846Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Capture installation text\n# %%capture\n# Import resnet50\nmodel = models.resnet50(pretrained=True)\n# Freeze model params \nfor param in model.parameters():\n    param.required_grad = False\n# Pull final fc layer feature dimensions\nfeatures = model.fc.in_features\n\n\n# Build custom classifier which reduces Resnets 1000 out_features to 6\nclassifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(features, 512)),\n                                        ('relu', nn.ReLU()),\n                                        ('drop', nn.Dropout(0.05)),\n                                        ('fc2', nn.Linear(512, 6)),\n                                        ]))\n\n# ('output', nn.LogSoftmax(dim=1)) - NLLLoss\n# Appending classifier layer to Resnet\nmodel.classifier = classifier\n# Pushing the model to cuda\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T08:57:39.524883Z","iopub.execute_input":"2024-06-30T08:57:39.525877Z","iopub.status.idle":"2024-06-30T08:57:40.184722Z","shell.execute_reply.started":"2024-06-30T08:57:39.525849Z","shell.execute_reply":"2024-06-30T08:57:40.183795Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define criterion and optimizer\ncriterion = nn.CrossEntropyLoss()\n# Pass the optimizer to the appended classifier layer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n# Set scheduler\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 15], gamma=0.05)\n# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=10, epochs=20)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T08:57:40.186013Z","iopub.execute_input":"2024-06-30T08:57:40.186312Z","iopub.status.idle":"2024-06-30T08:57:40.192949Z","shell.execute_reply.started":"2024-06-30T08:57:40.186288Z","shell.execute_reply":"2024-06-30T08:57:40.19195Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 20\n\n\ntr_losses = []\navg_epoch_tr_loss = []\ntr_accuracy = []\n\n\nval_losses = []\navg_epoch_val_loss = []\nval_accuracy = []\nval_loss_min = np.Inf\n\nfor epoch in range(epochs):\n    model.train()\n    for i, batch in enumerate(train_loader):\n        # Pull the data and labels from the batch\n        data, label = batch\n        # If available push data and label to GPU\n        #    if train_on_gpu:\n        data, label = data.to(device), label.to(device)\n        # Clear the gradient\n        optimizer.zero_grad()\n        # Compute the logit\n        logit = model(data)\n        # Compte loss\n        loss = criterion(logit, label)\n        # Backpropagate the gradients (accumulte the partial derivatives of loss)\n        loss.backward()\n        # Apply the updates to the optimizer step in the opposite direction to the gradient\n        optimizer.step()\n        # Store the losses of each batch\n        # loss.item() seperates the loss from comp graph\n        tr_losses.append(loss.item())\n        # Detach and store the average accuracy of each batch\n        tr_accuracy.append(label.eq(logit.argmax(dim=1)).float().mean())\n        # Print the rolling batch training loss every 20 batches\n    #     if i % 40 == 0:\n    #       print(f'Batch No: {i} \\tAverage Training Batch Loss: {torch.tensor(tr_losses).mean():.2f}')\n    # Print the average loss for each epoch\n    print(\n        f\"\\nEpoch No: {epoch + 1},Training Loss: {torch.tensor(tr_losses).mean():.2f}\"\n    )\n    # Print the average accuracy for each epoch\n    print(\n        f\"Epoch No: {epoch + 1}, Training Accuracy: {torch.tensor(tr_accuracy).mean():.2f}\\n\"\n    )\n    # Store the avg epoch loss for plotting\n    avg_epoch_tr_loss.append(torch.tensor(tr_losses).mean())\n\n    model.eval()\n    for i, batch in enumerate(val_loader):\n        # Pull the data and labels from the batch\n        data, label = batch\n        # If available push data and label to GPU\n        data, label = data.to(device), label.to(device)\n        # Compute the logits without computing the gradients\n        with torch.no_grad():\n            logit = model(data)\n        # Compte loss\n        loss = criterion(logit, label)\n        # Store test loss\n        val_losses.append(loss.item())\n        # Store the accuracy for each batch\n        val_accuracy.append(label.eq(logit.argmax(dim=1)).float().mean())\n        # if i % 40 == 0:\n        # print(f'Batch No: {i} \\tAverage Val Batch Loss: {torch.tensor(val_losses).mean():.2f}')\n    # Print the average loss for each epoch\n    print(\n        f\"\\nEpoch No: {epoch + 1}, Epoch Val Loss: {torch.tensor(val_losses).mean():.2f}\"\n    )\n    # Print the average accuracy for each epoch\n    print(\n        f\"Epoch No: {epoch + 1}, Epoch Val Accuracy: {torch.tensor(val_accuracy).mean():.2f}\\n\"\n    )\n    # Store the avg epoch loss for plotting\n    avg_epoch_val_loss.append(torch.tensor(val_losses).mean())\n\n    # Checpoininting the model using val loss threshold\n    if torch.tensor(val_losses).float().mean() <= val_loss_min:\n        print(\"Val Loss Decreased... Saving model\")\n        # save current model\n        torch.save(model.state_dict(), \"./model_state.pt\")\n        val_loss_min = torch.tensor(val_losses).mean()\n    # Step the scheduler for the next epoch\n    scheduler.step()\n    # Print the updated learning rate\n    print(\n        \"Learning Rate Set To: {:.10f}\".format(\n            optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n        ),\n        \"\\n\",\n    )","metadata":{"execution":{"iopub.status.busy":"2024-06-30T08:57:40.194487Z","iopub.execute_input":"2024-06-30T08:57:40.194904Z","iopub.status.idle":"2024-06-30T08:58:49.887447Z","shell.execute_reply.started":"2024-06-30T08:57:40.194871Z","shell.execute_reply":"2024-06-30T08:58:49.88649Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Magnitude weight Pruning in Pytorch\n\nPytorch provides support for implementing different pruning techniques. We will use the pytorch module for implementing model pruning.\n\nRef : https://pytorch.org/tutorials/intermediate/pruning_tutorial.html","metadata":{}},{"cell_type":"markdown","source":"Firstly, we will calculate the sparsity existing in resnet50 model. For this task, we won't be considering any bias parameters.","metadata":{}},{"cell_type":"markdown","source":"## Global pruning","metadata":{}},{"cell_type":"code","source":"def calc_global_sparsity_resnet50(model):\n    layers = [\n        model.conv1,\n        *[l.conv1 for l in model.layer1], *[l.conv2 for l in model.layer1], *[l.conv3 for l in model.layer1], *[l.downsample[0] for l in model.layer1 if l.downsample is not None],\n        *[l.conv1 for l in model.layer2], *[l.conv2 for l in model.layer2], *[l.conv3 for l in model.layer2], *[l.downsample[0] for l in model.layer2 if l.downsample is not None],\n        *[l.conv1 for l in model.layer3], *[l.conv2 for l in model.layer3], *[l.conv3 for l in model.layer3], *[l.downsample[0] for l in model.layer3 if l.downsample is not None],\n        *[l.conv1 for l in model.layer4], *[l.conv2 for l in model.layer4], *[l.conv3 for l in model.layer4], *[l.downsample[0] for l in model.layer4 if l.downsample is not None],\n        model.fc,\n        model.classifier.fc1,\n        model.classifier.fc2\n    ]\n    \n    num_zero_weights = sum((layer.weight == 0).sum().item() for layer in layers)\n    total_weights = sum(layer.weight.nelement() for layer in layers)\n    \n    return 100. * num_zero_weights / total_weights\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T08:58:49.90248Z","iopub.execute_input":"2024-06-30T08:58:49.903017Z","iopub.status.idle":"2024-06-30T08:58:49.916173Z","shell.execute_reply.started":"2024-06-30T08:58:49.902984Z","shell.execute_reply":"2024-06-30T08:58:49.915377Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"sparsity of the model before pruning: {:.2f}%\".format(calc_global_sparsity_resnet50(model)))","metadata":{"execution":{"iopub.status.busy":"2024-06-30T09:02:02.302434Z","iopub.execute_input":"2024-06-30T09:02:02.303427Z","iopub.status.idle":"2024-06-30T09:02:02.315407Z","shell.execute_reply.started":"2024-06-30T09:02:02.303385Z","shell.execute_reply":"2024-06-30T09:02:02.314247Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we will prune overall 20% of weights from the finetuned model.","metadata":{}},{"cell_type":"code","source":"parameters_to_prune = [\n        (model.conv1, 'weight'),\n        *[(l.conv1, 'weight') for l in model.layer1], *[(l.conv2, 'weight') for l in model.layer1], *[(l.conv3, 'weight') for l in model.layer1], *[(l.downsample[0], 'weight') for l in model.layer1 if l.downsample is not None],\n        *[(l.conv1, 'weight') for l in model.layer2], *[(l.conv2, 'weight') for l in model.layer2], *[(l.conv3, 'weight') for l in model.layer2], *[(l.downsample[0], 'weight') for l in model.layer2 if l.downsample is not None],\n        *[(l.conv1, 'weight') for l in model.layer3], *[(l.conv2, 'weight') for l in model.layer3], *[(l.conv3, 'weight') for l in model.layer3], *[(l.downsample[0], 'weight') for l in model.layer3 if l.downsample is not None],\n        *[(l.conv1, 'weight') for l in model.layer4], *[(l.conv2, 'weight') for l in model.layer4], *[(l.conv3, 'weight') for l in model.layer4], *[(l.downsample[0], 'weight') for l in model.layer4 if l.downsample is not None],\n        (model.fc, 'weight'),\n        (model.classifier.fc1, 'weight'),\n        (model.classifier.fc2, 'weight')\n    ]\n\n# Apply pruning\nprune.global_unstructured(\n    parameters_to_prune,\n    pruning_method=prune.L1Unstructured,\n    amount=0.2,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T09:02:06.983023Z","iopub.execute_input":"2024-06-30T09:02:06.984099Z","iopub.status.idle":"2024-06-30T09:02:07.045711Z","shell.execute_reply.started":"2024-06-30T09:02:06.984058Z","shell.execute_reply":"2024-06-30T09:02:07.044664Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Sparsity after pruning","metadata":{}},{"cell_type":"markdown","source":"Let's analyse how much percentage of parameters are pruned away layerwise","metadata":{}},{"cell_type":"code","source":"print(\n    \"Sparsity in conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.conv1.weight == 0))\n        / float(model.conv1.weight.nelement())\n    )\n)\nprint(\"==\"*80)\nprint(\n    \"Sparsity in layer1 first bottle neck conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer1[0].conv1.weight == 0))\n        /float(model.layer1[0].conv1.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer1 first bottle neck conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer1[0].conv2.weight == 0))\n        /float(model.layer1[0].conv2.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer1 first bottle neck conv3.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer1[0].conv3.weight == 0))\n        /float(model.layer1[0].conv3.weight.nelement())\n    )\n)\nprint(\n    \"Sparsity in layer1 first bottle neck downsample Conv2d.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer1[0].downsample[0].weight == 0))\n        /float(model.layer1[0].downsample[0].weight.nelement())\n    )\n)\nprint(\" \"*100)\nprint(\n    \"Sparsity in layer1 second bottle neck conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer1[1].conv1.weight == 0))\n        /float(model.layer1[1].conv1.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer1 second bottle neck conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer1[1].conv2.weight == 0))\n        /float(model.layer1[1].conv2.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer1 second bottle neck conv3.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer1[1].conv3.weight == 0))\n        /float(model.layer1[1].conv3.weight.nelement())\n    )\n)\n\nprint(\" \"*100)\nprint(\n    \"Sparsity in layer1 third bottle neck conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer1[2].conv1.weight == 0))\n        /float(model.layer1[2].conv1.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer1 third bottle neck conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer1[2].conv2.weight == 0))\n        /float(model.layer1[2].conv2.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer1 third bottle neck conv3.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer1[2].conv3.weight == 0))\n        /float(model.layer1[2].conv3.weight.nelement())\n    )\n)\nprint(\"==\"*80)\nprint(\n    \"Sparsity in layer2 first bottle neck conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer2[0].conv1.weight == 0))\n        /float(model.layer2[0].conv1.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer2 first bottle neck conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer2[0].conv2.weight == 0))\n        /float(model.layer2[0].conv2.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer2 first bottle neck conv3.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer2[0].conv3.weight == 0))\n        /float(model.layer2[0].conv3.weight.nelement())\n    )\n)\nprint(\n    \"Sparsity in layer2 first bottle neck downsample Conv2d.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer2[0].downsample[0].weight == 0))\n        /float(model.layer2[0].downsample[0].weight.nelement())\n    )\n)\nprint(\" \"*100)\nprint(\n    \"Sparsity in layer2 second bottle neck conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer2[1].conv1.weight == 0))\n        /float(model.layer2[1].conv1.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer2 second bottle neck conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer2[1].conv2.weight == 0))\n        /float(model.layer2[1].conv2.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer2 second bottle neck conv3.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer2[1].conv3.weight == 0))\n        /float(model.layer2[1].conv3.weight.nelement())\n    )\n)\n\nprint(\" \"*100)\nprint(\n    \"Sparsity in layer2 third bottle neck conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer2[2].conv1.weight == 0))\n        /float(model.layer2[2].conv1.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer2 third bottle neck conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer2[2].conv2.weight == 0))\n        /float(model.layer2[2].conv2.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer2 third bottle neck conv3.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer2[2].conv3.weight == 0))\n        /float(model.layer2[2].conv3.weight.nelement())\n    )\n)\n\nprint(\" \"*100)\nprint(\n    \"Sparsity in layer2 fourth bottle neck conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer2[3].conv1.weight == 0))\n        /float(model.layer2[3].conv1.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer2 fourth bottle neck conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer2[3].conv2.weight == 0))\n        /float(model.layer2[3].conv2.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer2 fourth bottle neck conv3.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer2[3].conv3.weight == 0))\n        /float(model.layer2[3].conv3.weight.nelement())\n    )\n)\n\nprint(\"==\"*80)\nprint(\n    \"Sparsity in layer3 first bottle neck conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[0].conv1.weight == 0))\n        /float(model.layer3[0].conv1.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer3 first bottle neck conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[0].conv2.weight == 0))\n        /float(model.layer3[0].conv2.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer3 first bottle neck conv3.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[0].conv3.weight == 0))\n        /float(model.layer3[0].conv3.weight.nelement())\n    )\n)\nprint(\n    \"Sparsity in layer3 first bottle neck downsample Conv2d.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[0].downsample[0].weight == 0))\n        /float(model.layer3[0].downsample[0].weight.nelement())\n    )\n)\nprint(\" \"*100)\nprint(\n    \"Sparsity in layer3 second bottle neck conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[1].conv1.weight == 0))\n        /float(model.layer3[1].conv1.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer3 second bottle neck conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[1].conv2.weight == 0))\n        /float(model.layer3[1].conv2.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer3 second bottle neck conv3.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[1].conv3.weight == 0))\n        /float(model.layer3[1].conv3.weight.nelement())\n    )\n)\n\nprint(\" \"*100)\nprint(\n    \"Sparsity in layer3 third bottle neck conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[2].conv1.weight == 0))\n        /float(model.layer3[2].conv1.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer3 third bottle neck conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[2].conv2.weight == 0))\n        /float(model.layer3[2].conv2.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer3 third bottle neck conv3.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[2].conv3.weight == 0))\n        /float(model.layer3[2].conv3.weight.nelement())\n    )\n)\n\nprint(\" \"*100)\nprint(\n    \"Sparsity in layer3 fourth bottle neck conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[3].conv1.weight == 0))\n        /float(model.layer3[3].conv1.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer3 fourth bottle neck conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[3].conv2.weight == 0))\n        /float(model.layer3[3].conv2.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer3 fourth bottle neck conv3.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[3].conv3.weight == 0))\n        /float(model.layer3[3].conv3.weight.nelement())\n    )\n)\n\nprint(\" \"*100)\nprint(\n    \"Sparsity in layer3 fifth bottle neck conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[4].conv1.weight == 0))\n        /float(model.layer3[4].conv1.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer3 fifth bottle neck conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[4].conv2.weight == 0))\n        /float(model.layer3[4].conv2.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer3 fifth bottle neck conv3.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[4].conv3.weight == 0))\n        /float(model.layer3[4].conv3.weight.nelement())\n    )\n)\n\nprint(\" \"*100)\nprint(\n    \"Sparsity in layer3 sixth bottle neck conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[5].conv1.weight == 0))\n        /float(model.layer3[5].conv1.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer3 sixth bottle neck conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[5].conv2.weight == 0))\n        /float(model.layer3[5].conv2.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer3 sixth bottle neck conv3.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer3[5].conv3.weight == 0))\n        /float(model.layer3[5].conv3.weight.nelement())\n    )\n)\n\nprint(\"==\"*80)\nprint(\n    \"Sparsity in layer4 first bottle neck conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer4[0].conv1.weight == 0))\n        /float(model.layer4[0].conv1.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer4 first bottle neck conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer4[0].conv2.weight == 0))\n        /float(model.layer4[0].conv2.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer4 first bottle neck conv3.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer4[0].conv3.weight == 0))\n        /float(model.layer4[0].conv3.weight.nelement())\n    )\n)\nprint(\n    \"Sparsity in layer4 first bottle neck downsample Conv2d.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer4[0].downsample[0].weight == 0))\n        /float(model.layer4[0].downsample[0].weight.nelement())\n    )\n)\nprint(\" \"*100)\nprint(\n    \"Sparsity in layer4 second bottle neck conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer4[1].conv1.weight == 0))\n        /float(model.layer4[1].conv1.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer4 second bottle neck conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer4[1].conv2.weight == 0))\n        /float(model.layer4[1].conv2.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer4 second bottle neck conv3.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer4[1].conv3.weight == 0))\n        /float(model.layer4[1].conv3.weight.nelement())\n    )\n)\n\nprint(\" \"*100)\nprint(\n    \"Sparsity in layer4 third bottle neck conv1.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer4[2].conv1.weight == 0))\n        /float(model.layer4[2].conv1.weight.nelement())\n    )\n)\n\nprint(\n    \"Sparsity in layer4 third bottle neck conv2.weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.layer4[2].conv2.weight == 0))\n        /float(model.layer4[2].conv2.weight.nelement())\n    )\n)\n\nprint(\"==\"*80)\nprint(\n    \"Sparsity in first linear weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.fc.weight == 0))\n        /float(model.fc.weight.nelement())\n    )\n)\nprint(\n    \"Sparsity in second linear weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.classifier.fc1.weight == 0))\n        /float(model.classifier.fc1.weight.nelement())\n    )\n)\nprint(\n    \"Sparsity in third linear weight: {:.2f}%\".format(\n        100. * float(torch.sum(model.classifier.fc2.weight == 0))\n        /float(model.classifier.fc2.weight.nelement())\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T09:02:20.045398Z","iopub.execute_input":"2024-06-30T09:02:20.045772Z","iopub.status.idle":"2024-06-30T09:02:20.114587Z","shell.execute_reply.started":"2024-06-30T09:02:20.045742Z","shell.execute_reply":"2024-06-30T09:02:20.113625Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Sparsity of the model after pruning: {:.2f}%\".format(calc_global_sparsity_resnet50(model)))","metadata":{"execution":{"iopub.status.busy":"2024-06-30T09:02:33.161883Z","iopub.execute_input":"2024-06-30T09:02:33.162262Z","iopub.status.idle":"2024-06-30T09:02:33.172888Z","shell.execute_reply.started":"2024-06-30T09:02:33.162231Z","shell.execute_reply":"2024-06-30T09:02:33.17196Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fine tuning using the pruned model\n\nWe will fine-tune the model with the pruned weights.","metadata":{}},{"cell_type":"code","source":"epochs = 20\n\n\ntr_losses = []\navg_epoch_tr_loss = []\ntr_accuracy = []\n\n\nval_losses = []\navg_epoch_val_loss = []\nval_accuracy = []\nval_loss_min = np.Inf\n\n\nfor epoch in range(epochs):\n    model.train()\n    for i, batch in enumerate(train_loader):\n        # Pull the data and labels from the batch\n        data, label = batch\n        # If available push data and label to GPU\n        #    if train_on_gpu:\n        data, label = data.to(device), label.to(device)\n        # Clear the gradient\n        optimizer.zero_grad()\n        # Compute the logit\n        logit = model(data)\n        # Compte loss\n        loss = criterion(logit, label)\n        # Backpropagate the gradients (accumulte the partial derivatives of loss)\n        loss.backward()\n        # Apply the updates to the optimizer step in the opposite direction to the gradient\n        optimizer.step()\n        # Store the losses of each batch\n        # loss.item() seperates the loss from comp graph\n        tr_losses.append(loss.item())\n        # Detach and store the average accuracy of each batch\n        tr_accuracy.append(label.eq(logit.argmax(dim=1)).float().mean())\n        # Print the rolling batch training loss every 20 batches\n    #     if i % 40 == 0:\n    #       print(f'Batch No: {i} \\tAverage Training Batch Loss: {torch.tensor(tr_losses).mean():.2f}')\n    # Print the average loss for each epoch\n    print(\n        f\"\\nEpoch No: {epoch + 1},Training Loss: {torch.tensor(tr_losses).mean():.2f}\"\n    )\n    # Print the average accuracy for each epoch\n    print(\n        f\"Epoch No: {epoch + 1}, Training Accuracy: {torch.tensor(tr_accuracy).mean():.2f}\\n\"\n    )\n    # Store the avg epoch loss for plotting\n    avg_epoch_tr_loss.append(torch.tensor(tr_losses).mean())\n\n    model.eval()\n    for i, batch in enumerate(val_loader):\n        # Pull the data and labels from the batch\n        data, label = batch\n        # If available push data and label to GPU\n        data, label = data.to(device), label.to(device)\n        # Compute the logits without computing the gradients\n        with torch.no_grad():\n            logit = model(data)\n        # Compte loss\n        loss = criterion(logit, label)\n        # Store test loss\n        val_losses.append(loss.item())\n        # Store the accuracy for each batch\n        val_accuracy.append(label.eq(logit.argmax(dim=1)).float().mean())\n    #     if i % 40 == 0:\n    #       print(f'Batch No: {i} \\tAverage Val Batch Loss: {torch.tensor(val_losses).mean():.2f}')\n    # Print the average loss for each epoch\n    print(\n        f\"\\nEpoch No: {epoch + 1}, Epoch Val Loss: {torch.tensor(val_losses).mean():.2f}\"\n    )\n    # Print the average accuracy for each epoch\n    print(\n        f\"Epoch No: {epoch + 1}, Epoch Val Accuracy: {torch.tensor(val_accuracy).mean():.2f}\\n\"\n    )\n    # Store the avg epoch loss for plotting\n    avg_epoch_val_loss.append(torch.tensor(val_losses).mean())\n\n    # Checpoininting the model using val loss threshold\n    if torch.tensor(val_losses).float().mean() <= val_loss_min:\n        print(\"Val Loss Decreased... Saving model\")\n        # save current model\n        torch.save(model.state_dict(), \"./model_state.pt\")\n        val_loss_min = torch.tensor(val_losses).mean()\n    # Step the scheduler for the next epoch\n    scheduler.step()\n    # Print the updated learning rate\n    print(\n        \"Learning Rate Set To: {:.10f}\".format(\n            optimizer.state_dict()[\"param_groups\"][0][\"lr\"]\n        ),\n        \"\\n\",\n    )","metadata":{"execution":{"iopub.status.busy":"2024-06-30T09:02:35.833742Z","iopub.execute_input":"2024-06-30T09:02:35.834521Z","iopub.status.idle":"2024-06-30T09:03:40.004727Z","shell.execute_reply.started":"2024-06-30T09:02:35.834487Z","shell.execute_reply":"2024-06-30T09:03:40.00352Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We  can observe that the **fine-tuned + pruned+ again fine-tuned model** performs almost as good as the **fine-tuned dense model**. If the desired performance is not obtained or if there is a need for more sparsity, one can try this process iteratively, till it acheives the desired result.","metadata":{}},{"cell_type":"markdown","source":"Now, we will make the pruning effect permanent using 'prune.remove' function.","metadata":{}},{"cell_type":"code","source":"modules = [\n        model.conv1,\n        *[l.conv1 for l in model.layer1], *[l.conv2 for l in model.layer1], *[l.conv3 for l in model.layer1], *[l.downsample[0] for l in model.layer1 if l.downsample is not None],\n        *[l.conv1 for l in model.layer2], *[l.conv2 for l in model.layer2], *[l.conv3 for l in model.layer2], *[l.downsample[0] for l in model.layer2 if l.downsample is not None],\n        *[l.conv1 for l in model.layer3], *[l.conv2 for l in model.layer3], *[l.conv3 for l in model.layer3], *[l.downsample[0] for l in model.layer3 if l.downsample is not None],\n        *[l.conv1 for l in model.layer4], *[l.conv2 for l in model.layer4], *[l.conv3 for l in model.layer4], *[l.downsample[0] for l in model.layer4 if l.downsample is not None],\n        model.fc,\n        model.classifier.fc1,\n        model.classifier.fc2\n    ]\n\nfor module in modules:\n    prune.remove(module, 'weight')","metadata":{"execution":{"iopub.status.busy":"2024-06-30T09:03:40.006546Z","iopub.execute_input":"2024-06-30T09:03:40.006889Z","iopub.status.idle":"2024-06-30T09:03:40.020987Z","shell.execute_reply.started":"2024-06-30T09:03:40.006862Z","shell.execute_reply":"2024-06-30T09:03:40.020188Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To confirm,let's check the sparsity of the pruned+fine-tuned model:","metadata":{}},{"cell_type":"code","source":"print(\"Sparsity of the model after pruning and fine tuning: {:.2f}%\".format(calc_global_sparsity_resnet50(model)))","metadata":{"execution":{"iopub.status.busy":"2024-06-30T09:04:16.652568Z","iopub.execute_input":"2024-06-30T09:04:16.653436Z","iopub.status.idle":"2024-06-30T09:04:16.663932Z","shell.execute_reply.started":"2024-06-30T09:04:16.653403Z","shell.execute_reply":"2024-06-30T09:04:16.66305Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# End Notes\n\n* This script demonstrates maginitude weight pruning on a finetuned resnet-50 model \n* We were able to obtain 20% sparsity in one iteration without much performance drop\n","metadata":{}}]}