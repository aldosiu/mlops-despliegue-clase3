{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":269359,"sourceType":"datasetVersion","datasetId":111880}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Model compression\n\nDeep neural networks are the state-of-the-art models for many tasks, especially with unstructured data like images, text etc. One issue with deep learning models is that they are highly overparamterized, making it difficult to deploy/use it in devices with low memory requirements. Compress the neural networks model without sacrifying the model performance is an active reasearch topic in machine learning. Parameter pruning,quantization,knowledge distillation etc are the widely used approaches nowadays to make the neural networks compact. In this notebook, we will demonstrate a knowledge distillation method and tries to incorporate a Resnet152 learned knowledge into a comparatively smaller Resnet50 model.","metadata":{}},{"cell_type":"markdown","source":"## Knowledge Distillation\n\nMagnitude pruning is one of the popular methods used for model compression. I have demonstrated one such pruning technique, in another [notebook](https://www.kaggle.com/code/neerajmohan/magnitude-weight-pruning-on-resnet50). Though very simple to implement, pruning methods have a limitation that the original model and the pruned model will almost have the same model architecture. The \"model compression\" relies on the hardware capabilities to handle the sparse parameters. The main advantage of knowledge distillation is that we can try to encorporate the knowledge of any trained model architecture to any other model architecture.\n\nThe idea of knowledge distillation is to use a larger model, known as teacher, to guide the training of a smaller one, termed as student model. This approach can be helpful when:\n\n1) There are specific hardware constraints when deploying a machine learning model <br>\n2) New state-of-the-art models are developed very often and we want to try to improve the performance of the deployed model without affecting the hardware requirements\n\nA simple but popular knowledge distillation approach was proposed in this [paper](https://arxiv.org/abs/1503.02531).\n\nFor a classification problem, neural networks produce class probabilites using a \"softmax\" output layer that converts the logits.\nLet there be total $C$ classes. For each class '$c$', the probability '$q_c$' is calculated by comparing '$z_c$' with the other logits.\n\n**$$q_c = {\\exp (z_{c} / T) \\over \\sum_{{i \\in C}} \\exp (z_{i} / T)} $$**\n\nNormally, the temperature '$T$' is set to 1. When we use higher temperature, we will have a softer probability distribution over classes. One way to transfer the knowledge from the trained teacher model to student model is to train the student model with an additional loss along with the cross entropy loss, which is called distillation loss function, along with a temperature, on the difference between the soft student predictions and the soft teacher labels.\n\n\nIn this notebook, we will use a finetuned ResNet152 model as teacher model and a resent50 model as student. We will compare the performance of a student resnet50 model with a normally fine-tuned resnet50 model.","metadata":{}},{"cell_type":"markdown","source":"# Problem statement\n\nThis is a multiclass image classification problem. There data contains images from 6 categories 'buildings','forest','glacier','mountain','sea','street'. The aim is to develop a machine learning model that correctly classifies an input image into one of the categories.\n\nIn this notebook, we try to finetune a resnet50 model from a learned resnet152 model, without lossing much expressivity of the later.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n#import keras","metadata":{"execution":{"iopub.status.busy":"2023-11-02T21:30:18.372036Z","iopub.execute_input":"2023-11-02T21:30:18.372775Z","iopub.status.idle":"2023-11-02T21:30:21.957124Z","shell.execute_reply.started":"2023-11-02T21:30:18.372739Z","shell.execute_reply":"2023-11-02T21:30:21.955739Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"#data-path\ntrain_dir = \"../input/intel-image-classification/seg_train/seg_train/\"\ntest_dir = \"../input/intel-image-classification/seg_test/seg_test/\"\n\n#data-configs\nbatch_size = 32\nimg_height = 150\nimg_width = 150","metadata":{"execution":{"iopub.status.busy":"2023-11-02T21:30:21.95996Z","iopub.execute_input":"2023-11-02T21:30:21.961312Z","iopub.status.idle":"2023-11-02T21:30:21.968369Z","shell.execute_reply.started":"2023-11-02T21:30:21.961257Z","shell.execute_reply":"2023-11-02T21:30:21.966693Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load train data\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n  train_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)\n\n# Load test data\n\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n  test_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T21:30:21.970747Z","iopub.execute_input":"2023-11-02T21:30:21.971987Z","iopub.status.idle":"2023-11-02T21:30:28.116736Z","shell.execute_reply.started":"2023-11-02T21:30:21.971865Z","shell.execute_reply":"2023-11-02T21:30:28.11577Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### A glimpse of train data","metadata":{}},{"cell_type":"code","source":"class_names = train_ds.class_names\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T21:30:28.118853Z","iopub.execute_input":"2023-11-02T21:30:28.119164Z","iopub.status.idle":"2023-11-02T21:30:29.483466Z","shell.execute_reply.started":"2023-11-02T21:30:28.119137Z","shell.execute_reply":"2023-11-02T21:30:29.482384Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training the Teacher model - using Transfer learning\n\nRef: https://keras.io/guides/transfer_learning/","metadata":{}},{"cell_type":"code","source":"base_model = keras.applications.ResNet152(\n    weights='imagenet',  # Load weights pre-trained on ImageNet.\n    input_shape=(img_height, img_width, 3),\n    include_top=False)  # Do not include the ImageNet classifier at the top.\nbase_model.trainable = False\ninputs = keras.Input(shape=(img_height, img_width, 3))\n# We make sure that the base_model is running in inference mode here,\n# by passing `training=False`. This is important for fine-tuning, as you will\n# learn in a few paragraphs.\nx = base_model(inputs, training=False)\n# Convert features of shape `base_model.output_shape[1:]` to vectors\nx = keras.layers.GlobalAveragePooling2D()(x)\n# A Dense classifier with a single unit (binary classification)\noutputs = keras.layers.Dense(6)(x)\nmodel = keras.Model(inputs, outputs)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T21:30:29.484759Z","iopub.execute_input":"2023-11-02T21:30:29.485094Z","iopub.status.idle":"2023-11-02T21:30:35.978229Z","shell.execute_reply.started":"2023-11-02T21:30:29.485065Z","shell.execute_reply":"2023-11-02T21:30:35.977248Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\nepochs = 20\nmodel.fit(train_ds, epochs=epochs)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T21:30:35.979706Z","iopub.execute_input":"2023-11-02T21:30:35.980112Z","iopub.status.idle":"2023-11-02T21:39:50.281837Z","shell.execute_reply.started":"2023-11-02T21:30:35.980077Z","shell.execute_reply":"2023-11-02T21:39:50.280744Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.compile(\n#     optimizer=keras.optimizers.Adam(),\n#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n#     metrics=[keras.metrics.SparseCategoricalAccuracy()],\n# )\n#print(y_pred)\nresults = model.evaluate(test_ds)\nprint(f\"Test accuracy with trained teacher model:{results[1]*100 :.2f} %\")\n#keras.metrics.SparseCategoricalAccuracy(y_pred,test_ds)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T21:39:50.283086Z","iopub.execute_input":"2023-11-02T21:39:50.283378Z","iopub.status.idle":"2023-11-02T21:39:55.152998Z","shell.execute_reply.started":"2023-11-02T21:39:50.283352Z","shell.execute_reply":"2023-11-02T21:39:55.152001Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Knowledge Distillation","metadata":{}},{"cell_type":"markdown","source":"Code reference : https://colab.research.google.com/drive/1Vo5rFF5JyHdJGFW88io4t5QS6q1klYPD?usp=sharing#scrollTo=Jxg7IWuJLB9g","metadata":{}},{"cell_type":"code","source":"\nclass Distiller(keras.Model):\n    def __init__(self, student, teacher):\n        super(Distiller, self).__init__()\n        self.teacher = teacher\n        self.student = student\n\n    def compile(\n        self,\n        optimizer,\n        metrics,\n        student_loss_fn,\n        distillation_loss_fn,\n        alpha=0.1,\n        temperature=3,\n    ):\n        \"\"\" Configure the distiller.\n\n        Args:\n            optimizer: Keras optimizer for the student weights\n            metrics: Keras metrics for evaluation\n            student_loss_fn: Loss function of difference between student\n                predictions and ground-truth\n            distillation_loss_fn: Loss function of difference between soft\n                student predictions and soft teacher predictions\n            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n            temperature: Temperature for softening probability distributions.\n                Larger temperature gives softer distributions.\n        \"\"\"\n        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n    def train_step(self, data):\n        # Unpack data\n        x, y = data\n\n        # Forward pass of teacher\n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n            # Forward pass of student\n            student_predictions = self.student(x, training=True)\n\n            # Compute losses\n            student_loss = self.student_loss_fn(y, student_predictions)\n            distillation_loss = self.distillation_loss_fn(\n                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n            )\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n\n        # Compute gradients\n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # Update the metrics configured in `compile()`.\n        self.compiled_metrics.update_state(y, student_predictions)\n\n        # Return a dict of performance\n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n        )\n        return results\n\n    def test_step(self, data):\n        # Unpack the data\n        x, y = data\n\n        # Compute predictions\n        y_prediction = self.student(x, training=False)\n\n        # Calculate the loss\n        student_loss = self.student_loss_fn(y, y_prediction)\n\n        # Update the metrics.\n        self.compiled_metrics.update_state(y, y_prediction)\n\n        # Return a dict of performance\n        results = {m.name: m.result() for m in self.metrics}\n        results.update({\"student_loss\": student_loss})\n        return results\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T21:39:55.154617Z","iopub.execute_input":"2023-11-02T21:39:55.155179Z","iopub.status.idle":"2023-11-02T21:39:55.16997Z","shell.execute_reply.started":"2023-11-02T21:39:55.155139Z","shell.execute_reply":"2023-11-02T21:39:55.168999Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"student_base_model = keras.applications.ResNet50(\n    weights='imagenet',  # Load weights pre-trained on ImageNet.\n    input_shape=(img_height, img_width, 3),\n    include_top=False)  # Do not include the ImageNet classifier at the top.\nstudent_base_model.trainable = True #Fine-tune all the weights of the model\ninputs_student = keras.Input(shape=(150, 150, 3))\n# We make sure that the base_model is running in inference mode here,\n# by passing `training=False`. This is important for fine-tuning, as you will\n# learn in a few paragraphs.\nx_student = student_base_model(inputs_student, training=True)\n# Convert features of shape `base_model.output_shape[1:]` to vectors\nx_student = keras.layers.GlobalAveragePooling2D()(x_student)\n# A Dense classifier with a single unit (binary classification)\noutputs_student = keras.layers.Dense(6)(x_student)\nstudent = keras.Model(inputs_student, outputs_student)\nstudent.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T21:39:55.171461Z","iopub.execute_input":"2023-11-02T21:39:55.171893Z","iopub.status.idle":"2023-11-02T21:40:00.786277Z","shell.execute_reply.started":"2023-11-02T21:39:55.171859Z","shell.execute_reply":"2023-11-02T21:40:00.785234Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize and compile distiller\ndistiller = Distiller(student=student, teacher=model)\ndistiller.compile(\n    optimizer=keras.optimizers.Adam(),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    distillation_loss_fn=keras.losses.KLDivergence(),\n    alpha=0.05,\n    temperature=40,\n)\n\n# Distill teacher to student\ndistiller.fit(train_ds, epochs=10)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T21:40:00.789215Z","iopub.execute_input":"2023-11-02T21:40:00.789511Z","iopub.status.idle":"2023-11-02T21:50:45.855333Z","shell.execute_reply.started":"2023-11-02T21:40:00.789486Z","shell.execute_reply":"2023-11-02T21:50:45.854276Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate student on test dataset\nresults = distiller.evaluate(test_ds)\nprint(f\"Test accuracy of the student model, with distilled knowledge from teacher model:{results[0]*100 :.2f} %\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T21:57:23.823107Z","iopub.execute_input":"2023-11-02T21:57:23.823432Z","iopub.status.idle":"2023-11-02T21:57:24.716201Z","shell.execute_reply.started":"2023-11-02T21:57:23.8234Z","shell.execute_reply":"2023-11-02T21:57:24.715242Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can observe that the student model is performing comparatively as good as the teacher model.\n\nNow, we will check the performance of student model architecture (resnet50 in this case), when it is trained without any knowledge distillation technique.","metadata":{}},{"cell_type":"code","source":"student_scratch_base_model = keras.applications.ResNet50(\n    weights='imagenet',  # Load weights pre-trained on ImageNet.\n    input_shape=(img_height, img_width, 3),\n    include_top=False)  # Do not include the ImageNet classifier at the top.\nstudent_scratch_base_model.trainable = True\ninputs_scratch = keras.Input(shape=(img_height, img_width, 3))\n# We make sure that the base_model is running in inference mode here,\n# by passing `training=False`. This is important for fine-tuning, as you will\n# learn in a few paragraphs.\nx_student = student_scratch_base_model(inputs_scratch, training=True)\n# Convert features of shape `base_model.output_shape[1:]` to vectors\nx_student = keras.layers.GlobalAveragePooling2D()(x_student)\n# A Dense classifier with a single unit (binary classification)\noutputs_scratch = keras.layers.Dense(6)(x_student)\nstudent_scratch = keras.Model(inputs_scratch, outputs_scratch)\n#student_scratch = keras.Model(inputs, outputs)\nstudent_scratch.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T21:50:47.849201Z","iopub.execute_input":"2023-11-02T21:50:47.849973Z","iopub.status.idle":"2023-11-02T21:50:50.508398Z","shell.execute_reply.started":"2023-11-02T21:50:47.84993Z","shell.execute_reply":"2023-11-02T21:50:50.507421Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train student as done usually\nstudent_scratch.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)\n\n# Train and evaluate student trained from scratch.\nstudent_scratch.fit(train_ds, epochs=10)\nresult = student_scratch.evaluate(test_ds)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T21:50:50.509752Z","iopub.execute_input":"2023-11-02T21:50:50.51008Z","iopub.status.idle":"2023-11-02T21:57:23.813178Z","shell.execute_reply.started":"2023-11-02T21:50:50.510052Z","shell.execute_reply":"2023-11-02T21:57:23.812292Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Test accuracy of the student model, when finetuned without any knowledge distillation :{result[1]*100 :.2f} %\")","metadata":{"execution":{"iopub.status.busy":"2023-11-02T21:57:23.814431Z","iopub.execute_input":"2023-11-02T21:57:23.814748Z","iopub.status.idle":"2023-11-02T21:57:23.820354Z","shell.execute_reply.started":"2023-11-02T21:57:23.814719Z","shell.execute_reply":"2023-11-02T21:57:23.819239Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can clearly observe that for the same number of epochs and same architecture, student model with knowledge distillation is better than the model without any knowledge distillation technique.","metadata":{}},{"cell_type":"markdown","source":"# End notes\n\n1) The notebook demonstrates a simple knowledge distillation technique <br>\n2) The method aims to train a comparatively smaller resnet50 model that incorporates the knowledge of a mighty resnet152 model","metadata":{}}]}